import pandas as pd
import csv
import os
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Selenium ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--enable-unsafe-swiftshader")
options.add_argument("--ignore-gpu-blocklist")
options.add_argument("--disable-software-rasterizer")

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

CHECKPOINT_FILE = "processed_isbns.txt"
CSV_CHECKPOINT_FILE = "processed_csv_files.txt"


# ì´ë¯¸ í¬ë¡¤ë§í•œ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
def load_processed_csv_files():
    if os.path.exists(CSV_CHECKPOINT_FILE):
        with open(CSV_CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return set(f.read().splitlines())
    return set()


# í¬ë¡¤ë§ ì™„ë£Œí•œ CSV íŒŒì¼ ì €ì¥í•˜ê¸°
def save_processed_csv_file(file_path):
    with open(CSV_CHECKPOINT_FILE, "a", encoding="utf-8") as f:
        f.write(file_path + "\n")


# ì´ë¯¸ í¬ë¡¤ë§í•œ ISBN ë¶ˆëŸ¬ì˜¤ê¸°
def load_processed_isbns():
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return set(f.read().splitlines())
    return set()


# í¬ë¡¤ë§í•œ ISBN ì €ì¥í•˜ê¸°
def save_processed_isbn(isbn):
    with open(CHECKPOINT_FILE, "a", encoding="utf-8") as f:
        f.write(isbn + "\n")


# ISBN ëª©ë¡ ì½ê¸° (NA ê°’ ì œê±° ë° ì¤‘ë³µ ì œê±°)
def read_isbn_list(filename):
    try:
        data = pd.read_csv(filename, low_memory=False, dtype={"ISBN_THIRTEEN_NO": str})
        isbn_list = data['ISBN_THIRTEEN_NO'].dropna().astype(str).tolist()
        unique_isbn_list = list(dict.fromkeys(isbn_list))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
        return unique_isbn_list
    except Exception as e:
        print(f"[ERROR] ISBN CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []


# ISBNìœ¼ë¡œ ë„ì„œ ì¡°íšŒ ë° URL ë°˜í™˜
def get_book_page_url(book_isbn):
    # search_url = f"https://search.kyobobook.co.kr/web/search?vPstrKeyWord={book_isbn}&orderClick=LAG"
    try:
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"[ERROR] ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ë©”ì‹œì§€ í™•ì¸
    no_data_tag = soup.find("div", class_="no_data size_sm")
    if no_data_tag:
        print(f"[INFO] ISBN {book_isbn} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. í¬ë¡¤ë§ ê±´ë„ˆëœ€.")
        return None

    # ì •ìƒì ì¸ ë„ì„œ ìƒì„¸ í˜ì´ì§€ ë§í¬ ê°€ì ¸ì˜¤ê¸°
    book_detail_link = soup.find("a", class_="prod_link")
    return book_detail_link["href"] if book_detail_link else None


# ë„ì„œ ì •ë³´ í¬ë¡¤ë§
def get_book_info(book_url, driver):
    if not book_url:
        return None

    try:
        driver.get(book_url)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "basic_info")))
    except Exception as e:
        print(f"[ERROR] ë„ì„œ ì •ë³´ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # ì œëª©
    title_tag = soup.find("span", class_="prod_title")
    title = title_tag.text.strip() if title_tag else "N/A"

    # ì¶œíŒì‚¬
    publisher_tag = soup.find("a", class_="btn_publish_link")
    publisher = publisher_tag.text.strip() if publisher_tag else "N/A"

    # ì´ë¯¸ì§€ URL
    img_div = soup.find("div", class_="blur_img_wrap portrait")
    img_url = img_div.find("img").get("src") if img_div and img_div.find("img") else "N/A"

    # ì¹´í…Œê³ ë¦¬
    category_tags = soup.find_all(class_="intro_category_link")
    categories = [tag.get_text(strip=True) for tag in category_tags] if category_tags else []
    category = list(set([item for category in categories for item in category.split("/")])) if categories else ["N/A"]

    # ë„ì„œ ì†Œê°œ
    intro_tag = soup.find("div", class_="intro_bottom")
    intro_list = intro_tag.find_all("div", class_="info_text") if intro_tag else []
    intro = ''.join([intro.text.strip() for intro in intro_list]) if intro_list else "N/A"

    # ì¶œíŒì‚¬ ì„œí‰
    book_publish_review_div = soup.find("div", class_="book_publish_review")
    book_publish_review_p = book_publish_review_div.find("p", class_="info_text") if book_publish_review_div else None
    book_publish_review = book_publish_review_p.text.strip() if book_publish_review_p else "N/A"

    # ëª©ì°¨ ê°€ì ¸ì˜¤ê¸°
    book_contents_div = soup.find("div", class_="book_contents")
    book_contents_li = book_contents_div.find("li", class_="book_contents_item") if book_contents_div else None
    book_contents = book_contents_li.text.strip() if book_contents_li else "N/A"

    # ìª½ìˆ˜
    page_num = "N/A"
    basic_info_div = soup.find("div", class_="basic_info")
    if basic_info_div:
        for tr in basic_info_div.find_all("tr"):
            th = tr.find("th")
            if th and th.text.strip() == "ìª½ìˆ˜":
                td = tr.find("td")
                page_num = td.text.strip()[:-1] if td else "N/A"
                break

    print(f"[INFO] í¬ë¡¤ë§ ì™„ë£Œ: {title}")

    return {
        "Title": title,
        "Publisher": publisher,
        "Image": img_url,
        "Category": ", ".join(category),
        "Intro": intro,
        "Review": book_publish_review,
        "Contents": book_contents,
        "Page": page_num
    }


# CSV ì €ì¥ í•¨ìˆ˜ (ì—°ë„ë³„ & CSV íŒŒì¼ë³„ ì €ì¥)
def save_to_csv(data, year, filename):
    try:
        save_folder = os.path.join("kyobo_crawling_data", year)
        os.makedirs(save_folder, exist_ok=True)

        save_path = os.path.join(save_folder, filename)
        file_exists = os.path.isfile(save_path)

        with open(save_path, mode="a", newline="", encoding="utf-8") as file:
            writer = csv.DictWriter(file, fieldnames=["Title", "Publisher", "Image", "Category", "Intro", "Review",
                                                      "Contents", "Page"])
            if not file_exists:
                writer.writeheader()
            writer.writerows(data)
        print(f"[INFO] CSV ì €ì¥ ì™„ë£Œ: {save_path} (ì´ {len(data)}ê¶Œ ì €ì¥ë¨)")
    except Exception as e:
        print(f"[ERROR] CSV ì €ì¥ ì‹¤íŒ¨ ({filename}): {e}")


# main í•¨ìˆ˜ (CSV ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ ì¶”ê°€)
def main():
    base_folder = "processing_book_data"
    processed_isbns = load_processed_isbns()
    processed_csv_files = load_processed_csv_files()

    for year in os.listdir(base_folder):
        if year == "2019":
            print(f"[INFO] {year} í´ë” ê±´ë„ˆëœ€")
            continue

        year_path = os.path.join(base_folder, year)
        if os.path.isdir(year_path):
            print(f"\nğŸ“‚ {year} í´ë”")
            csv_files = [f for f in os.listdir(year_path) if f.endswith(".csv")]

            for file in csv_files:
                file_path = os.path.join(year_path, file)

                # ì´ë¯¸ í¬ë¡¤ë§ ì™„ë£Œí•œ íŒŒì¼ì´ë©´ ê±´ë„ˆë›°ê¸°
                if file_path in processed_csv_files:
                    print(f"[INFO] {file} ì´ë¯¸ í¬ë¡¤ë§ ì™„ë£Œë¨. ê±´ë„ˆëœ€.")
                    continue

                print(f"\nğŸ“– {file} í¬ë¡¤ë§ ì‹œì‘")
                isbn_list = read_isbn_list(file_path)
                driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
                book_data = []

                for idx, isbn in enumerate(isbn_list, start=1):
                    if not isbn or isbn in processed_isbns:
                        continue

                    save_processed_isbn(isbn)  # isbn ì €ì¥

                    print(f"[INFO] {idx}/{len(isbn_list)} ISBN ì²˜ë¦¬ ì¤‘: {isbn}")
                    book_url = get_book_page_url(isbn)

                    if book_url:
                        book_info = get_book_info(book_url, driver)
                        if book_info:
                            book_data.append(book_info)

                    if idx % 100 == 0:
                        save_to_csv(book_data, year, file)
                        book_data = []

                driver.quit()
                save_to_csv(book_data, year, file)

                # CSV íŒŒì¼ í¬ë¡¤ë§ ì™„ë£Œ í›„ ê¸°ë¡
                save_processed_csv_file(file_path)


if __name__ == "__main__":
    main()
